{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RagaAI Catalyst components for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst.tracers import Tracer\n",
    "from ragaai_catalyst import RagaAICatalyst, init_tracing\n",
    "import os\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=os.getenv(\"RAGAAI_CATALYST_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"RAGAAI_CATALYST_SECRET_KEY\"),\n",
    "    base_url=os.getenv(\"RAGAAI_CATALYST_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# Initialize tracer\n",
    "tracer = Tracer(\n",
    "    project_name=\"llama_index_testing\",\n",
    "    dataset_name=\"router_query_engine\",\n",
    "    tracer_type=\"llamaindex\",\n",
    ")\n",
    "\n",
    "init_tracing(catalyst=catalyst, tracer=tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.base.base_selector import SelectorResult\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE\n",
    "\n",
    "\n",
    "class QueryEngineSelectionEvent(Event):\n",
    "    \"\"\"Result of selecting the query engine tools.\"\"\"\n",
    "\n",
    "    selected_query_engines: SelectorResult\n",
    "\n",
    "\n",
    "class SynthesizeEvent(Event):\n",
    "    \"\"\"Event for synthesizing the response from different query engines.\"\"\"\n",
    "\n",
    "    result: List[RESPONSE_TYPE]\n",
    "    selected_query_engines: SelectorResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.selectors.utils import get_selector_from_llm\n",
    "from llama_index.core.base.response.schema import (\n",
    "    PydanticResponse,\n",
    "    Response,\n",
    "    AsyncStreamingResponse,\n",
    ")\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class RouterQueryEngineWorkflow(Workflow):\n",
    "    @step\n",
    "    async def selector(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> QueryEngineSelectionEvent:\n",
    "        \"\"\"\n",
    "        Selects a single/ multiple query engines based on the query.\n",
    "        \"\"\"\n",
    "\n",
    "        await ctx.set(\"query\", ev.get(\"query\"))\n",
    "        await ctx.set(\"llm\", ev.get(\"llm\"))\n",
    "        await ctx.set(\"query_engine_tools\", ev.get(\"query_engine_tools\"))\n",
    "        await ctx.set(\"summarizer\", ev.get(\"summarizer\"))\n",
    "\n",
    "        llm = Settings.llm\n",
    "        select_multiple_query_engines = ev.get(\"select_multi\")\n",
    "        query = ev.get(\"query\")\n",
    "        query_engine_tools = ev.get(\"query_engine_tools\")\n",
    "\n",
    "        selector = get_selector_from_llm(\n",
    "            llm, is_multi=select_multiple_query_engines\n",
    "        )\n",
    "\n",
    "        query_engines_metadata = [\n",
    "            query_engine.metadata for query_engine in query_engine_tools\n",
    "        ]\n",
    "\n",
    "        selected_query_engines = await selector.aselect(\n",
    "            query_engines_metadata, query\n",
    "        )\n",
    "\n",
    "        return QueryEngineSelectionEvent(\n",
    "            selected_query_engines=selected_query_engines\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def generate_responses(\n",
    "        self, ctx: Context, ev: QueryEngineSelectionEvent\n",
    "    ) -> SynthesizeEvent:\n",
    "        \"\"\"Generate the responses from the selected query engines.\"\"\"\n",
    "\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        selected_query_engines = ev.selected_query_engines\n",
    "        query_engine_tools = await ctx.get(\"query_engine_tools\")\n",
    "\n",
    "        query_engines = [engine.query_engine for engine in query_engine_tools]\n",
    "\n",
    "        print(\n",
    "            f\"number of selected query engines: {len(selected_query_engines.selections)}\"\n",
    "        )\n",
    "\n",
    "        if len(selected_query_engines.selections) > 1:\n",
    "            tasks = []\n",
    "            for selected_query_engine in selected_query_engines.selections:\n",
    "                print(\n",
    "                    f\"Selected query engine: {selected_query_engine.index}: {selected_query_engine.reason}\"\n",
    "                )\n",
    "                query_engine = query_engines[selected_query_engine.index]\n",
    "                tasks.append(query_engine.aquery(query))\n",
    "\n",
    "            response_generated = await asyncio.gather(*tasks)\n",
    "\n",
    "        else:\n",
    "            query_engine = query_engines[\n",
    "                selected_query_engines.selections[0].index\n",
    "            ]\n",
    "\n",
    "            print(\n",
    "                f\"Selected query engine: {selected_query_engines.ind}: {selected_query_engines.reason}\"\n",
    "            )\n",
    "\n",
    "            response_generated = [await query_engine.aquery(query)]\n",
    "\n",
    "        return SynthesizeEvent(\n",
    "            result=response_generated,\n",
    "            selected_query_engines=selected_query_engines,\n",
    "        )\n",
    "\n",
    "    async def acombine_responses(\n",
    "        self,\n",
    "        summarizer: TreeSummarize,\n",
    "        responses: List[RESPONSE_TYPE],\n",
    "        query_bundle: QueryBundle,\n",
    "    ) -> RESPONSE_TYPE:\n",
    "        \"\"\"Async combine multiple response from sub-engines.\"\"\"\n",
    "\n",
    "        print(\"Combining responses from multiple query engines.\")\n",
    "\n",
    "        response_strs = []\n",
    "        source_nodes = []\n",
    "        for response in responses:\n",
    "            if isinstance(\n",
    "                response, (AsyncStreamingResponse, PydanticResponse)\n",
    "            ):\n",
    "                response_obj = await response.aget_response()\n",
    "            else:\n",
    "                response_obj = response\n",
    "            source_nodes.extend(response_obj.source_nodes)\n",
    "            response_strs.append(str(response))\n",
    "\n",
    "        summary = await summarizer.aget_response(\n",
    "            query_bundle.query_str, response_strs\n",
    "        )\n",
    "\n",
    "        if isinstance(summary, str):\n",
    "            return Response(response=summary, source_nodes=source_nodes)\n",
    "        elif isinstance(summary, BaseModel):\n",
    "            return PydanticResponse(\n",
    "                response=summary, source_nodes=source_nodes\n",
    "            )\n",
    "        else:\n",
    "            return AsyncStreamingResponse(\n",
    "                response_gen=summary, source_nodes=source_nodes\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    async def synthesize_responses(\n",
    "        self, ctx: Context, ev: SynthesizeEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Synthesizes the responses from the generated responses.\"\"\"\n",
    "\n",
    "        response_generated = ev.result\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        summarizer = await ctx.get(\"summarizer\")\n",
    "        selected_query_engines = ev.selected_query_engines\n",
    "\n",
    "        if len(response_generated) > 1:\n",
    "            response = await self.acombine_responses(\n",
    "                summarizer, response_generated, QueryBundle(query_str=query)\n",
    "            )\n",
    "        else:\n",
    "            response = response_generated[0]\n",
    "\n",
    "        response.metadata = response.metadata or {}\n",
    "        response.metadata[\"selector_result\"] = selected_query_engines\n",
    "\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.default_prompt_selectors import (\n",
    "    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n",
    ")\n",
    "\n",
    "summarizer = TreeSummarize(\n",
    "    llm=llm,\n",
    "    summary_template=DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at data/paul_graham/paul_graham_essay.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "directory = 'data/paul_graham/'\n",
    "file_path = os.path.join(directory, 'paul_graham_essay.txt')\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "url = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt'\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print(f\"File saved at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    ")\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "keyword_index = SimpleKeywordTableIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "list_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "keyword_query_engine = keyword_index.as_query_engine()\n",
    "\n",
    "list_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=list_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Paul Graham eassy on\"\n",
    "        \" What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from Paul Graham essay on What\"\n",
    "        \" I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "keyword_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=keyword_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context using keywords from Paul\"\n",
    "        \" Graham essay on What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "query_engine_tools = [list_tool, vector_tool, keyword_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[(\"Provide the summary of the document?\",True),\n",
    "           (\"What did the author do growing up?\",False),\n",
    "           (\"What were noteable events and people from the authors time at Interleaf and YC?,\",True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with RagaAI Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected query engines: 1\n",
      "Selected query engine: 0: This choice directly addresses the need for a summary of the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading traces...\n",
      "**********\n",
      "Trace: query\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: Provide the summary of the document?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: The document is a reflective essay detailing the author's journey through various phases of life, focusing on their experiences in writing, programming, art, and entrepreneurship. It begins with their early interests in writing short stories and programming on an IBM 1401, transitioning to microcomputers and eventually to a deeper engagement with artificial intelligence during college. The author expresses disillusionment with philosophy and a shift towards AI, leading to self-directed learning and the development of a thesis on SHRDLU.\n",
       "\n",
       "After graduate school, the author explores art, attending classes at Harvard and later the Accademia di Belle Arti in Florence, where they realize the limitations of formal art education. They return to the U.S. to work at Interleaf, where they learn about the software industry and the dynamics of technology companies. This experience culminates in the founding of Viaweb, an early e-commerce platform, which is later acquired by Yahoo.\n",
       "\n",
       "The essay also discusses the author's transition to angel investing and the founding of Y Combinator, a startup accelerator that revolutionizes startup funding through a batch model. The author reflects on the challenges and successes of Y Combinator, the importance of community among startups, and the evolution of their own interests over time.\n",
       "\n",
       "In the latter part of the essay, the author shares their return to painting after stepping back from Y Combinator, the development of a new Lisp programming language called Bel, and the realization of the potential for online publishing. The narrative emphasizes the value of pursuing unprestigious work, the significance of personal experiences in shaping one's path, and the ongoing exploration of creativity and technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected query engines: 1\n",
      "Selected query engine: 1: The question asks for specific context about the author's experiences growing up, which aligns with retrieving specific context from the essay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading traces...\n",
      "**********\n",
      "Trace: query\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: What did the author do growing up?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: Growing up, the author worked on writing and programming outside of school. Initially, he wrote short stories, which he later described as lacking plot and depth. He also began programming on an IBM 1401 in 9th grade, where he experimented with an early version of Fortran, although he found it puzzling and struggled to create meaningful programs. Eventually, he got a TRS-80 microcomputer, which allowed him to start programming more seriously, creating simple games and a word processor. Despite enjoying programming, he initially planned to study philosophy in college."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.core.indices.keyword_table.retrievers:> Starting query: What were noteable events and people from the authors time at Interleaf and YC?,\n",
      "INFO:llama_index.core.indices.keyword_table.retrievers:query keywords: ['interleaf', 'people', 'yc', 'time', 'authors', 'noteable', 'events']\n",
      "INFO:llama_index.core.indices.keyword_table.retrievers:> Extracted keywords: ['interleaf', 'people', 'yc', 'time']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected query engines: 2\n",
      "Selected query engine: 1: This choice is useful for retrieving specific context related to notable events and people from the author's time at Interleaf and YC.\n",
      "Selected query engine: 2: This choice allows for retrieving specific context using keywords, which can help in identifying notable events and people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading traces...\n",
      "**********\n",
      "Trace: query\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "**********\n",
      "Combining responses from multiple query engines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: What were noteable events and people from the authors time at Interleaf and YC?,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: During the author's time at Interleaf, notable events included the decision to incorporate a scripting language inspired by Emacs, which was a dialect of Lisp. The author gained insights into the dynamics of technology companies, emphasizing the importance of being run by product people rather than salespeople, and learned about the challenges posed by bureaucratic customers and the need for a flexible work environment.\n",
       "\n",
       "At Y Combinator, significant events included the transition of leadership when the author handed over control to Sam Altman, following advice from Robert Morris. The growth of YC into a supportive community for startups was another key development, as alumni began to assist each other, fostering collaboration. Additionally, the author faced challenges related to managing disputes among cofounders and the stress associated with running Hacker News, which became a major focus during his tenure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traces uploaded\n"
     ]
    }
   ],
   "source": [
    "with tracer:\n",
    "    w = RouterQueryEngineWorkflow(timeout=200)\n",
    "\n",
    "    for query,select_multi in questions:\n",
    "        result = await w.run(\n",
    "            query=query,\n",
    "            llm=llm,\n",
    "            query_engine_tools=query_engine_tools,\n",
    "            summarizer=summarizer,\n",
    "            select_multi=select_multi, \n",
    "        )\n",
    "\n",
    "        display(\n",
    "            Markdown(\"> Question: {}\".format(query)),\n",
    "            Markdown(\"Answer: {}\".format(result)),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
