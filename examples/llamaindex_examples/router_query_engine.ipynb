{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RagaAI Catalyst components for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst.tracers import Tracer\n",
    "from ragaai_catalyst import RagaAICatalyst, init_tracing\n",
    "import os\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=os.getenv(\"RAGAAI_CATALYST_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"RAGAAI_CATALYST_SECRET_KEY\"),\n",
    "    base_url=os.getenv(\"RAGAAI_CATALYST_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# Initialize tracer\n",
    "tracer = Tracer(\n",
    "    project_name=\"llamaindex_tracing_examples\",\n",
    "    dataset_name=\"router_query_engine1\",\n",
    "    tracer_type=\"Agentic\",\n",
    ")\n",
    "\n",
    "init_tracing(catalyst=catalyst, tracer=tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.base.base_selector import SelectorResult\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.base.response.schema import RESPONSE_TYPE\n",
    "\n",
    "\n",
    "class QueryEngineSelectionEvent(Event):\n",
    "    \"\"\"Result of selecting the query engine tools.\"\"\"\n",
    "\n",
    "    selected_query_engines: SelectorResult\n",
    "\n",
    "\n",
    "class SynthesizeEvent(Event):\n",
    "    \"\"\"Event for synthesizing the response from different query engines.\"\"\"\n",
    "\n",
    "    result: List[RESPONSE_TYPE]\n",
    "    selected_query_engines: SelectorResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.selectors.utils import get_selector_from_llm\n",
    "from llama_index.core.base.response.schema import (\n",
    "    PydanticResponse,\n",
    "    Response,\n",
    "    AsyncStreamingResponse,\n",
    ")\n",
    "from llama_index.core.bridge.pydantic import BaseModel\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core import Settings\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import asyncio\n",
    "\n",
    "\n",
    "class RouterQueryEngineWorkflow(Workflow):\n",
    "    @step\n",
    "    async def selector(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> QueryEngineSelectionEvent:\n",
    "        \"\"\"\n",
    "        Selects a single/ multiple query engines based on the query.\n",
    "        \"\"\"\n",
    "\n",
    "        await ctx.set(\"query\", ev.get(\"query\"))\n",
    "        await ctx.set(\"llm\", ev.get(\"llm\"))\n",
    "        await ctx.set(\"query_engine_tools\", ev.get(\"query_engine_tools\"))\n",
    "        await ctx.set(\"summarizer\", ev.get(\"summarizer\"))\n",
    "\n",
    "        llm = Settings.llm\n",
    "        select_multiple_query_engines = ev.get(\"select_multi\")\n",
    "        query = ev.get(\"query\")\n",
    "        query_engine_tools = ev.get(\"query_engine_tools\")\n",
    "\n",
    "        selector = get_selector_from_llm(\n",
    "            llm, is_multi=select_multiple_query_engines\n",
    "        )\n",
    "\n",
    "        query_engines_metadata = [\n",
    "            query_engine.metadata for query_engine in query_engine_tools\n",
    "        ]\n",
    "\n",
    "        selected_query_engines = await selector.aselect(\n",
    "            query_engines_metadata, query\n",
    "        )\n",
    "\n",
    "        return QueryEngineSelectionEvent(\n",
    "            selected_query_engines=selected_query_engines\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def generate_responses(\n",
    "        self, ctx: Context, ev: QueryEngineSelectionEvent\n",
    "    ) -> SynthesizeEvent:\n",
    "        \"\"\"Generate the responses from the selected query engines.\"\"\"\n",
    "\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        selected_query_engines = ev.selected_query_engines\n",
    "        query_engine_tools = await ctx.get(\"query_engine_tools\")\n",
    "\n",
    "        query_engines = [engine.query_engine for engine in query_engine_tools]\n",
    "\n",
    "        print(\n",
    "            f\"number of selected query engines: {len(selected_query_engines.selections)}\"\n",
    "        )\n",
    "\n",
    "        if len(selected_query_engines.selections) > 1:\n",
    "            tasks = []\n",
    "            for selected_query_engine in selected_query_engines.selections:\n",
    "                print(\n",
    "                    f\"Selected query engine: {selected_query_engine.index}: {selected_query_engine.reason}\"\n",
    "                )\n",
    "                query_engine = query_engines[selected_query_engine.index]\n",
    "                tasks.append(query_engine.aquery(query))\n",
    "\n",
    "            response_generated = await asyncio.gather(*tasks)\n",
    "\n",
    "        else:\n",
    "            query_engine = query_engines[\n",
    "                selected_query_engines.selections[0].index\n",
    "            ]\n",
    "\n",
    "            print(\n",
    "                f\"Selected query engine: {selected_query_engines.ind}: {selected_query_engines.reason}\"\n",
    "            )\n",
    "\n",
    "            response_generated = [await query_engine.aquery(query)]\n",
    "\n",
    "        return SynthesizeEvent(\n",
    "            result=response_generated,\n",
    "            selected_query_engines=selected_query_engines,\n",
    "        )\n",
    "\n",
    "    async def acombine_responses(\n",
    "        self,\n",
    "        summarizer: TreeSummarize,\n",
    "        responses: List[RESPONSE_TYPE],\n",
    "        query_bundle: QueryBundle,\n",
    "    ) -> RESPONSE_TYPE:\n",
    "        \"\"\"Async combine multiple response from sub-engines.\"\"\"\n",
    "\n",
    "        print(\"Combining responses from multiple query engines.\")\n",
    "\n",
    "        response_strs = []\n",
    "        source_nodes = []\n",
    "        for response in responses:\n",
    "            if isinstance(\n",
    "                response, (AsyncStreamingResponse, PydanticResponse)\n",
    "            ):\n",
    "                response_obj = await response.aget_response()\n",
    "            else:\n",
    "                response_obj = response\n",
    "            source_nodes.extend(response_obj.source_nodes)\n",
    "            response_strs.append(str(response))\n",
    "\n",
    "        summary = await summarizer.aget_response(\n",
    "            query_bundle.query_str, response_strs\n",
    "        )\n",
    "\n",
    "        if isinstance(summary, str):\n",
    "            return Response(response=summary, source_nodes=source_nodes)\n",
    "        elif isinstance(summary, BaseModel):\n",
    "            return PydanticResponse(\n",
    "                response=summary, source_nodes=source_nodes\n",
    "            )\n",
    "        else:\n",
    "            return AsyncStreamingResponse(\n",
    "                response_gen=summary, source_nodes=source_nodes\n",
    "            )\n",
    "\n",
    "    @step\n",
    "    async def synthesize_responses(\n",
    "        self, ctx: Context, ev: SynthesizeEvent\n",
    "    ) -> StopEvent:\n",
    "        \"\"\"Synthesizes the responses from the generated responses.\"\"\"\n",
    "\n",
    "        response_generated = ev.result\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        summarizer = await ctx.get(\"summarizer\")\n",
    "        selected_query_engines = ev.selected_query_engines\n",
    "\n",
    "        if len(response_generated) > 1:\n",
    "            response = await self.acombine_responses(\n",
    "                summarizer, response_generated, QueryBundle(query_str=query)\n",
    "            )\n",
    "        else:\n",
    "            response = response_generated[0]\n",
    "\n",
    "        response.metadata = response.metadata or {}\n",
    "        response.metadata[\"selector_result\"] = selected_query_engines\n",
    "\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts.default_prompt_selectors import (\n",
    "    DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n",
    ")\n",
    "\n",
    "summarizer = TreeSummarize(\n",
    "    llm=llm,\n",
    "    summary_template=DEFAULT_TREE_SUMMARIZE_PROMPT_SEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at data/paul_graham/paul_graham_essay.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "directory = 'data/paul_graham/'\n",
    "file_path = os.path.join(directory, 'paul_graham_essay.txt')\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "url = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt'\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print(f\"File saved at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SummaryIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    ")\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "keyword_index = SimpleKeywordTableIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "list_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "keyword_query_engine = keyword_index.as_query_engine()\n",
    "\n",
    "list_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=list_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to Paul Graham eassy on\"\n",
    "        \" What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from Paul Graham essay on What\"\n",
    "        \" I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "keyword_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=keyword_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context using keywords from Paul\"\n",
    "        \" Graham essay on What I Worked On.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "query_engine_tools = [list_tool, vector_tool, keyword_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RouterQueryEngineWorkflow(timeout=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide the summary of the document?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with RagaAI Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to patch Anthropic methods: ChatAnthropic.invoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of selected query engines: 1\n",
      "Selected query engine: 0: This choice directly addresses the need for a summary of the document.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: Provide the summary of the document?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: The document chronicles the journey of an individual who navigated various fields, including writing, programming, artificial intelligence, and art, before ultimately becoming a prominent figure in the startup ecosystem. It begins with reflections on early experiences in writing and programming during childhood, leading to a shift from philosophy to artificial intelligence in college. The narrative details the challenges faced in graduate school, the realization of the limitations of AI at the time, and a pivot towards Lisp programming.\n",
       "\n",
       "After a stint in the art world, the individual transitioned back to technology, co-founding Viaweb, an early e-commerce platform, which was later acquired by Yahoo. The experience at Viaweb led to the establishment of Y Combinator, an influential startup accelerator that revolutionized seed funding by supporting multiple startups simultaneously. The document highlights the evolution of Y Combinator, its unique batch model, and the community it fostered among founders.\n",
       "\n",
       "Throughout the narrative, themes of ambition, the pursuit of passion, and the importance of working on projects that may lack prestige are emphasized. The individual reflects on personal growth, the impact of mentorship, and the significance of writing essays as a means of exploration and expression. The document concludes with a return to painting and the development of a new Lisp dialect, Bel, showcasing a continuous cycle of learning and creativity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragaai_catalyst.tracers.agentic_tracing.utils.zip_list_of_unique_files: Zip file created successfully.\n",
      "INFO:ragaai_catalyst.tracers.agentic_tracing.tracers.base: Traces saved successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading agentic traces...\n",
      "Uploading code...\n",
      "Dataset trace code inserted successfully\n"
     ]
    }
   ],
   "source": [
    "with tracer:\n",
    "    result = await w.run(\n",
    "        query=query,\n",
    "        llm=llm,\n",
    "        query_engine_tools=query_engine_tools,\n",
    "        summarizer=summarizer,\n",
    "        select_multi=True, \n",
    "    )\n",
    "\n",
    "    display(\n",
    "        Markdown(\"> Question: {}\".format(query)),\n",
    "        Markdown(\"Answer: {}\".format(result)),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
