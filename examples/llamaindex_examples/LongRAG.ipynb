{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongRAG Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RagaAI Catalyst components for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst.tracers import Tracer\n",
    "from ragaai_catalyst import RagaAICatalyst, init_tracing\n",
    "import os\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=os.getenv(\"RAGAAI_CATALYST_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"RAGAAI_CATALYST_SECRET_KEY\"),\n",
    "    base_url=os.getenv(\"RAGAAI_CATALYST_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# Initialize tracer\n",
    "tracer = Tracer(\n",
    "    project_name=\"llamaindex_tracing_examples\",\n",
    "    dataset_name=\"Long_rag\",\n",
    "    tracer_type=\"Agentic\",\n",
    ")\n",
    "\n",
    "init_tracing(catalyst=catalyst, tracer=tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Set, FrozenSet\n",
    "\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "DEFAULT_CHUNK_SIZE = 4096  \n",
    "DEFAULT_MAX_GROUP_SIZE = 20  \n",
    "DEFAULT_SMALL_CHUNK_SIZE = 512 \n",
    "DEFAULT_TOP_K = 8  \n",
    "\n",
    "\n",
    "def split_doc(chunk_size: int, documents: List[BaseNode]) -> List[TextNode]:\n",
    "    text_parser = SentenceSplitter(chunk_size=chunk_size)\n",
    "    return text_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "\n",
    "def group_docs(\n",
    "    nodes: List[str],\n",
    "    adj: Dict[str, List[str]],\n",
    "    max_group_size: Optional[int] = DEFAULT_MAX_GROUP_SIZE,\n",
    ") -> Set[FrozenSet[str]]:\n",
    "    docs = sorted(nodes, key=lambda node: len(adj[node]))\n",
    "    groups = set()  \n",
    "    for d in docs:\n",
    "        related_groups = set()\n",
    "        for r in adj[d]:\n",
    "            for g in groups:\n",
    "                if r in g:\n",
    "                    related_groups = related_groups.union(frozenset([g]))\n",
    "\n",
    "        gnew = {d}\n",
    "        related_groupsl = sorted(related_groups, key=lambda el: len(el))\n",
    "        for g in related_groupsl:\n",
    "            if max_group_size is None or len(gnew) + len(g) <= max_group_size:\n",
    "                gnew = gnew.union(g)\n",
    "                if g in groups:\n",
    "                    groups.remove(g)\n",
    "\n",
    "        groups.add(frozenset(gnew))\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def get_grouped_docs(\n",
    "    nodes: List[TextNode],\n",
    "    max_group_size: Optional[int] = DEFAULT_MAX_GROUP_SIZE,\n",
    ") -> List[TextNode]:\n",
    "\n",
    "    nodes_str = [node.id_ for node in nodes]\n",
    "    adj: Dict[str, List[str]] = {\n",
    "        node.id_: [val.node_id for val in node.relationships.values()]\n",
    "        for node in nodes\n",
    "    }\n",
    "    nodes_dict = {node.id_: node for node in nodes}\n",
    "\n",
    "    res = group_docs(nodes_str, adj, max_group_size)\n",
    "\n",
    "    ret_nodes = []\n",
    "    for g in res:\n",
    "        cur_node = TextNode()\n",
    "\n",
    "        for node_id in g:\n",
    "            cur_node.text += nodes_dict[node_id].text + \"\\n\\n\"\n",
    "            cur_node.metadata.update(nodes_dict[node_id].metadata)\n",
    "\n",
    "        ret_nodes.append(cur_node)\n",
    "\n",
    "    return ret_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.vector_stores.simple import BasePydanticVectorStore\n",
    "from llama_index.core.schema import QueryBundle, NodeWithScore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "class LongRAGRetriever(BaseRetriever):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grouped_nodes: List[TextNode],\n",
    "        small_toks: List[TextNode],\n",
    "        vector_store: BasePydanticVectorStore,\n",
    "        similarity_top_k: int = DEFAULT_TOP_K,\n",
    "    ) -> None:\n",
    "\n",
    "        self._grouped_nodes = grouped_nodes\n",
    "        self._grouped_nodes_dict = {node.id_: node for node in grouped_nodes}\n",
    "        self._small_toks = small_toks\n",
    "        self._small_toks_dict = {node.id_: node for node in self._small_toks}\n",
    "\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        self._vec_store = vector_store\n",
    "        self._embed_model = Settings.embed_model\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding, similarity_top_k=500\n",
    "        )\n",
    "\n",
    "        query_res = self._vec_store.query(vector_store_query)\n",
    "\n",
    "        top_parents_set: Set[str] = set()\n",
    "        top_parents: List[NodeWithScore] = []\n",
    "        for id_, similarity in zip(query_res.ids, query_res.similarities):\n",
    "            cur_node = self._small_toks_dict[id_]\n",
    "            parent_id = cur_node.ref_doc_id\n",
    "            if parent_id not in top_parents_set:\n",
    "                top_parents_set.add(parent_id)\n",
    "\n",
    "                parent_node = self._grouped_nodes_dict[parent_id]\n",
    "                node_with_score = NodeWithScore(\n",
    "                    node=parent_node, score=similarity\n",
    "                )\n",
    "                top_parents.append(node_with_score)\n",
    "\n",
    "                if len(top_parents_set) >= self._similarity_top_k:\n",
    "                    break\n",
    "\n",
    "        assert len(top_parents) == min(\n",
    "            self._similarity_top_k, len(self._grouped_nodes)\n",
    "        )\n",
    "\n",
    "        return top_parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Any\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class LoadNodeEvent(Event):\n",
    "    small_nodes: Iterable[TextNode]\n",
    "    grouped_nodes: list[TextNode]\n",
    "    index: VectorStoreIndex\n",
    "    similarity_top_k: int\n",
    "    llm: LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    step,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    ")\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "class LongRAGWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def ingest(self, ev: StartEvent) -> LoadNodeEvent | None:\n",
    "\n",
    "        data_dir: str = ev.get(\"data_dir\")\n",
    "        llm: LLM = ev.get(\"llm\")\n",
    "        chunk_size: int | None = ev.get(\"chunk_size\")\n",
    "        similarity_top_k: int = ev.get(\"similarity_top_k\")\n",
    "        small_chunk_size: int = ev.get(\"small_chunk_size\")\n",
    "        index: VectorStoreIndex | None = ev.get(\"index\")\n",
    "        index_kwargs: dict[str, Any] | None = ev.get(\"index_kwargs\")\n",
    "\n",
    "        if any(\n",
    "            i is None\n",
    "            for i in [data_dir, llm, similarity_top_k, small_chunk_size]\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        if not index:\n",
    "            docs = SimpleDirectoryReader(data_dir).load_data()\n",
    "            if chunk_size is not None:\n",
    "                nodes = split_doc(\n",
    "                    chunk_size, docs\n",
    "                )  \n",
    "                grouped_nodes = get_grouped_docs(\n",
    "                    nodes\n",
    "                )  \n",
    "            else:\n",
    "                grouped_nodes = docs\n",
    "\n",
    "            small_nodes = split_doc(small_chunk_size, grouped_nodes)\n",
    "\n",
    "            index_kwargs = index_kwargs or {}\n",
    "            index = VectorStoreIndex(small_nodes, **index_kwargs)\n",
    "        else:\n",
    "            small_nodes = index.docstore.docs.values()\n",
    "            grouped_nodes = get_grouped_docs(small_nodes, None)\n",
    "\n",
    "        return LoadNodeEvent(\n",
    "            small_nodes=small_nodes,\n",
    "            grouped_nodes=grouped_nodes,\n",
    "            index=index,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            llm=llm,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def make_query_engine(\n",
    "        self, ctx: Context, ev: LoadNodeEvent\n",
    "    ) -> StopEvent:\n",
    "        retriever = LongRAGRetriever(\n",
    "            grouped_nodes=ev.grouped_nodes,\n",
    "            small_toks=ev.small_nodes,\n",
    "            similarity_top_k=ev.similarity_top_k,\n",
    "            vector_store=ev.index.vector_store,\n",
    "        )\n",
    "        query_eng = RetrieverQueryEngine.from_args(retriever, ev.llm)\n",
    "\n",
    "        return StopEvent(\n",
    "            result={\n",
    "                \"retriever\": retriever,\n",
    "                \"query_engine\": query_eng,\n",
    "                \"index\": ev.index,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def query(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        query_str: str | None = ev.get(\"query_str\")\n",
    "        query_eng = ev.get(\"query_eng\")\n",
    "\n",
    "        if query_str is None:\n",
    "            return None\n",
    "\n",
    "        result = query_eng.query(query_str)\n",
    "        return StopEvent(result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "wf = LongRAGWorkflow(timeout=60)\n",
    "llm = OpenAI(\"gpt-4o-mini\")\n",
    "data_dir = \"data\"\n",
    "\n",
    "result = await wf.run(\n",
    "data_dir=data_dir,\n",
    "llm=llm,\n",
    "chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "similarity_top_k=DEFAULT_TOP_K,\n",
    "small_chunk_size=DEFAULT_SMALL_CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with RagaAI Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to patch Anthropic methods: ChatAnthropic.invoke\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "IR, or Indistinguishability Rate, is a metric proposed to measure the quality of individual entries in generated datasets. It assesses how often a strong language model fails to identify a synthetically generated entry as being of lower quality when combined with a few real dataset entries. A high IR indicates that the generated data closely matches the properties of real-world data, suggesting that the discriminator is randomly guessing, while a low IR indicates that the generated data is out-of-distribution from real-world data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragaai_catalyst.tracers.agentic_tracing.utils.zip_list_of_unique_files: Zip file created successfully.\n",
      "INFO:ragaai_catalyst.tracers.agentic_tracing.tracers.base: Traces saved successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading agentic traces...\n",
      "Code already exists\n"
     ]
    }
   ],
   "source": [
    "with tracer:\n",
    "    res = await wf.run(\n",
    "        query_str=\"what is IR?\",\n",
    "        query_eng=result[\"query_engine\"],\n",
    "    )\n",
    "    display(Markdown(str(res)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
