{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiStep Query Engine Tracing using RagaAI Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import RagaAI Catalyst components for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token(s) set successfully\n"
     ]
    }
   ],
   "source": [
    "from ragaai_catalyst.tracers import Tracer\n",
    "from ragaai_catalyst import RagaAICatalyst, init_tracing\n",
    "import os\n",
    "\n",
    "catalyst = RagaAICatalyst(\n",
    "    access_key=os.getenv(\"RAGAAI_CATALYST_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"RAGAAI_CATALYST_SECRET_KEY\"),\n",
    "    base_url=os.getenv(\"RAGAAI_CATALYST_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# Initialize tracer\n",
    "tracer = Tracer(\n",
    "    project_name=\"llama_index_testing\",\n",
    "    dataset_name=\"multi_step_query_engine\",\n",
    "    tracer_type=\"llamaindex\",\n",
    ")\n",
    "\n",
    "init_tracing(catalyst=catalyst, tracer=tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from typing import Dict, List, Any\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class QueryMultiStepEvent(Event):\n",
    "    nodes: List[NodeWithScore]\n",
    "    source_nodes: List[NodeWithScore]\n",
    "    final_response_metadata: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.response_synthesizers import (\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "\n",
    "from llama_index.core.schema import QueryBundle, TextNode\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "from typing import cast\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "class MultiStepQueryEngineWorkflow(Workflow):\n",
    "    def combine_queries(\n",
    "        self,\n",
    "        query_bundle: QueryBundle,\n",
    "        prev_reasoning: str,\n",
    "        index_summary: str,\n",
    "        llm: LLM,\n",
    "    ) -> QueryBundle:\n",
    "        transform_metadata = {\n",
    "            \"prev_reasoning\": prev_reasoning,\n",
    "            \"index_summary\": index_summary,\n",
    "        }\n",
    "        return StepDecomposeQueryTransform(llm=llm)(\n",
    "            query_bundle, metadata=transform_metadata\n",
    "        )\n",
    "\n",
    "    def default_stop_fn(self, stop_dict: Dict) -> bool:\n",
    "        query_bundle = cast(QueryBundle, stop_dict.get(\"query_bundle\"))\n",
    "        if query_bundle is None:\n",
    "            raise ValueError(\"Response must be provided to stop function.\")\n",
    "\n",
    "        return \"none\" in query_bundle.query_str.lower()\n",
    "\n",
    "    @step\n",
    "    async def query_multistep(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> QueryMultiStepEvent:\n",
    "        prev_reasoning = \"\"\n",
    "        cur_response = None\n",
    "        should_stop = False\n",
    "        cur_steps = 0\n",
    "\n",
    "        final_response_metadata: Dict[str, Any] = {\"sub_qa\": []}\n",
    "\n",
    "        text_chunks = []\n",
    "        source_nodes = []\n",
    "\n",
    "        query = ev.get(\"query\")\n",
    "        await ctx.set(\"query\", ev.get(\"query\"))\n",
    "\n",
    "        llm = Settings.llm\n",
    "        stop_fn = self.default_stop_fn\n",
    "\n",
    "        num_steps = ev.get(\"num_steps\")\n",
    "        query_engine = ev.get(\"query_engine\")\n",
    "        index_summary = ev.get(\"index_summary\")\n",
    "\n",
    "        while not should_stop:\n",
    "            if num_steps is not None and cur_steps >= num_steps:\n",
    "                should_stop = True\n",
    "                break\n",
    "            elif should_stop:\n",
    "                break\n",
    "\n",
    "            updated_query_bundle = self.combine_queries(\n",
    "                QueryBundle(query_str=query),\n",
    "                prev_reasoning,\n",
    "                index_summary,\n",
    "                llm,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Created query for the step - {cur_steps} is: {updated_query_bundle}\"\n",
    "            )\n",
    "\n",
    "            stop_dict = {\"query_bundle\": updated_query_bundle}\n",
    "            if stop_fn(stop_dict):\n",
    "                should_stop = True\n",
    "                break\n",
    "\n",
    "            cur_response = query_engine.query(updated_query_bundle)\n",
    "\n",
    "            cur_qa_text = (\n",
    "                f\"\\nQuestion: {updated_query_bundle.query_str}\\n\"\n",
    "                f\"Answer: {cur_response!s}\"\n",
    "            )\n",
    "            text_chunks.append(cur_qa_text)\n",
    "            for source_node in cur_response.source_nodes:\n",
    "                source_nodes.append(source_node)\n",
    "            final_response_metadata[\"sub_qa\"].append(\n",
    "                (updated_query_bundle.query_str, cur_response)\n",
    "            )\n",
    "\n",
    "            prev_reasoning += (\n",
    "                f\"- {updated_query_bundle.query_str}\\n\" f\"- {cur_response!s}\\n\"\n",
    "            )\n",
    "            cur_steps += 1\n",
    "\n",
    "        nodes = [\n",
    "            NodeWithScore(node=TextNode(text=text_chunk))\n",
    "            for text_chunk in text_chunks\n",
    "        ]\n",
    "        return QueryMultiStepEvent(\n",
    "            nodes=nodes,\n",
    "            source_nodes=source_nodes,\n",
    "            final_response_metadata=final_response_metadata,\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def synthesize(\n",
    "        self, ctx: Context, ev: QueryMultiStepEvent\n",
    "    ) -> StopEvent:\n",
    "        response_synthesizer = get_response_synthesizer()\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        final_response = await response_synthesizer.asynthesize(\n",
    "            query=query,\n",
    "            nodes=ev.nodes,\n",
    "            additional_source_nodes=ev.source_nodes,\n",
    "        )\n",
    "        final_response.metadata = ev.final_response_metadata\n",
    "\n",
    "        return StopEvent(result=final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved at data/paul_graham/paul_graham_essay.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "directory = 'data/paul_graham/'\n",
    "file_path = os.path.join(directory, 'paul_graham_essay.txt')\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "url = 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt'\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print(f\"File saved at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"data/paul_graham\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3\n",
    "index_summary = \"Used to answer questions about the author\"\n",
    "query = \"In which city did the author found his first company, Viaweb?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the workflow with RagaAI Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created query for the step - 0 is: Who is the author who founded Viaweb?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading traces...\n",
      "**********\n",
      "Trace: query\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created query for the step - 1 is: In which city did the author of the text found Viaweb?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading traces...\n",
      "**********\n",
      "Trace: query\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created query for the step - 2 is: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "> Question: In which city did the author found his first company, Viaweb?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Answer: The text does not provide information on the city where the author founded his first company, Viaweb."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traces uploaded\n"
     ]
    }
   ],
   "source": [
    "with tracer:\n",
    "    w = MultiStepQueryEngineWorkflow(timeout=200)\n",
    "    result = await w.run(\n",
    "        query=query,\n",
    "        query_engine=query_engine,\n",
    "        index_summary=index_summary,\n",
    "        num_steps=num_steps,\n",
    "    )\n",
    "\n",
    "    display(\n",
    "        Markdown(\"> Question: {}\".format(query)),\n",
    "        Markdown(\"Answer: {}\".format(result)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[('Who is the author who founded Viaweb?', 'The author who founded Viaweb is not explicitly named in the context. However, it is implied that the author of the text, who also started a new company called Aspra, was a founder of Viaweb.'), ('In which city did the author of the text found Viaweb?', 'The text does not provide information on the city where the author founded Viaweb.')]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_qa = result.metadata[\"sub_qa\"]\n",
    "tuples = [(t[0], t[1].response) for t in sub_qa]\n",
    "display(Markdown(f\"{tuples}\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
